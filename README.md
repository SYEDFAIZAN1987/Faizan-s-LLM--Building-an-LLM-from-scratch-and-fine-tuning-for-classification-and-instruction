# ğŸš€ Faizan's LLM: Building a Large Language Model from scratch: 
**Then pretraining and fine-Tuning the Large Language Model for Classification and Instruction.**

## ğŸŒŸ Overview

This repository contains code and documentation for the Large Language Model I built from scratch. I then fine-tunied the LLM after pretraining the Transformer-based Large Language Models (LLMs). It covers essential topics such as:

- ğŸ† Fine-tuning for classification and instruction-following tasks
- ğŸ“š Pretraining a Transformer model from scratch
- ğŸ¯ Low-Rank Adaptation (LoRA) for efficient fine-tuning
- ğŸ“‰ Cosine decay learning rate scheduling
- ğŸš€ Gradient clipping for stable training
- âš¡ Detailed implementation of Transformer architectures

## ğŸ”¥ Features

- **ğŸ§  Fine-Tuning Classification Models**: Train a Transformer-based model on a classification dataset.
- **ğŸ“ Fine-Tuning for Instruction Following**: Optimize models to follow instructions using reinforcement learning.
- **ğŸ› ï¸ Pretraining from Scratch**: Build and train a Transformer model with custom tokenization.
- **ğŸ“Š LoRA Integration**: Implement LoRA for parameter-efficient fine-tuning.
- **ğŸ“ˆ Cosine Decay Scheduler**: Adjust learning rate dynamically for smooth convergence.
- **ğŸ›¡ï¸ Gradient Clipping**: Prevent exploding gradients during training.
- **âš™ï¸ Transformer Architecture**: Custom implementation of multi-head attention, layer normalization, and feed-forward networks.

## ğŸ“‚ Folder Structure

```
â”œâ”€â”€ finetuningclassification.ipynb  # Fine-tuning on classification tasks
â”œâ”€â”€ finetuninginstruction.ipynb     # Fine-tuning for instruction following
â”œâ”€â”€ pretraining.ipynb               # Pretraining a Transformer from scratch
â”œâ”€â”€ transformer.ipynb               # Transformer architecture implementation
â”œâ”€â”€ LLMcore.py                      # Core classes and functions for the LLM 
â”œâ”€â”€ README.md                        # Documentation
```

## ğŸ›  Installation

Ensure you have the required dependencies installed before running the notebooks.

```bash
pip install torch transformers datasets
```
ğŸ”¹ Fine-Tuning Classification ModelThe classification fine-tuning was performed using GPT-2 124M parameters, with pretrained weights loaded before fine-tuning.
Run the finetuningclassification.ipynb notebook to train a Transformer-based classifier.
ğŸ”¹ Fine-Tuning for Instruction FollowingThe instruction fine-tuning was performed on GPT-2 300M medium parameters, using pretrained weights loaded before fine-tuning.
Use the finetuninginstruction.ipynb notebook to fine-tune an LLM for instruction-following tasks.
## ğŸ“Œ Usage

### ğŸ”¹ Fine-Tuning Classification Model

Run the `finetuningclassification.ipynb` notebook to train a Transformer-based classifier.

### ğŸ”¹ Fine-Tuning for Instruction Following

Use the `finetuninginstruction.ipynb` notebook to fine-tune an LLM for instruction-following tasks.

### ğŸ”¹ Pretraining from Scratch

To pretrain a Transformer model from scratch, execute the `pretraining.ipynb` notebook.

### ğŸ”¹ Transformer Architecture

The `transformer.ipynb` notebook provides an in-depth implementation of Transformer blocks, including:

- ğŸ“Œ Token and positional embeddings
- ğŸ“Œ Multi-head self-attention
- ğŸ“Œ Layer normalization
- ğŸ“Œ Feed-forward networks
- ğŸ“Œ Residual connections

## ğŸ“Š Training Details

### ğŸ¯ Model Performance and Evaluation

- **âœ… Accuracy Scores for the classification fine tuned LLM**: 
  - Training Accuracy: **94.81%**
  - Validation Accuracy: **96.53%**
  - Test Accuracy: **92.57%**
 
  -  **âœ… Accuracy Scores for the instruction fine tuned LLM**: 
  - Accuracy Score: **45.84* as adjudicated by 'gpt 3.5 turbo' LLM model.
  - Room for improvement via modulation of the hyperparameters- learning rate, batch size, cosine decay and LoRA and model size.

  ![Training Accuracy](https://github.com/SYEDFAIZAN1987/Faizan-s-LLM--Building-an-LLM-from-scratch-and-fine-tuning-for-classification-and-instruction/blob/main/Accuracy%20score%20adjudicated%20by%20gpt%203.5%20turbo.png)

- **ğŸ“‰ Pretraining Loss Curve**:
  
  ![Pretraining Loss](https://github.com/SYEDFAIZAN1987/Faizan-s-LLM--Building-an-LLM-from-scratch-and-fine-tuning-for-classification-and-instruction/blob/main/Pretraining%20Loss.png)

- **ğŸ”¥ Temperature Scaling in Pretraining**:
  
  ![Temperature Scaling](https://github.com/SYEDFAIZAN1987/Faizan-s-LLM--Building-an-LLM-from-scratch-and-fine-tuning-for-classification-and-instruction/blob/main/Temperature%20scaling%20in%20pretraining.png)

- **ğŸ“Š Loss Curves for Classification Fine-Tuning**:
  
  ![Loss Curves](https://github.com/SYEDFAIZAN1987/Faizan-s-LLM--Building-an-LLM-from-scratch-and-fine-tuning-for-classification-and-instruction/blob/main/classification%20finetuning%20llm%20plot.png)

- **ğŸ” Classification Fine-Tuning Performance**:
  
  ![Classification Fine-Tuning](https://github.com/SYEDFAIZAN1987/Faizan-s-LLM--Building-an-LLM-from-scratch-and-fine-tuning-for-classification-and-instruction/blob/main/classification%20finetuning%20llm.png)

- **ğŸ“ Instruction Fine-Tuning Results**:
  
  ![Instruction Fine-Tuning](https://github.com/SYEDFAIZAN1987/Faizan-s-LLM--Building-an-LLM-from-scratch-and-fine-tuning-for-classification-and-instruction/blob/main/instruction%20finetuning%20llm%20plot.png)

### ğŸ“‰ Cosine Decay Learning Rate

The learning rate is adjusted using cosine decay for stable convergence.

### ğŸš€ Gradient Clipping

To prevent instability, gradients are clipped during backpropagation.

### ğŸ† Low-Rank Adaptation (LoRA)

LoRA is implemented to enable efficient fine-tuning with minimal computational cost.

## ğŸ“š References

- ğŸ“– Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. *Advances in Neural Information Processing Systems*, 30.
- ğŸ“– Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). Improving language understanding by generative pre-training.
- ğŸ“– Hu, E. J., Wang, Y., Singh, A., Wang, Z., Yu, K., & Ainsworth, S. (2021). LoRA: Low-Rank Adaptation of Large Language Models.
- ğŸ“– Sebastian Raschka. *Building an LLM from Scratch*.
- ğŸ“– Jay Alammar and Maarten Grootendorst. Hands-On Large Language Models: Language Understanding and Generation.
- ğŸ“– Andrej Karpathy. *Building a Chat-GPT*. Youtube
- ğŸ“– Krish Naik. *Machine Learning and Deep Learning Tutorials*. Youtube and Udemy.

## ğŸ’¡ Contributing

ğŸ‰ Contributions are welcome! Please feel free to submit issues or pull requests.

## ğŸ“œ License

This project is licensed under the **MIT License**.




