{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66dd524e-864c-4012-b0a2-ccfc56e80024",
   "metadata": {
    "id": "66dd524e-864c-4012-b0a2-ccfc56e80024"
   },
   "source": [
    "# Pretraining Faizan's LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92b989e9-da36-4159-b212-799184764dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.8.2\n",
      "numpy version: 1.26.4\n",
      "tiktoken version: 0.7.0\n",
      "torch version: 2.2.1+cu121\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"matplotlib\", \n",
    "        \"numpy\", \n",
    "        \"tiktoken\", \n",
    "        \"torch\",\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "da78fbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                         stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # optional projection\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)  # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "        decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "        print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "\n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86000d74-624a-48f0-86da-f41926cb9e04",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "86000d74-624a-48f0-86da-f41926cb9e04",
    "outputId": "ad482cfd-5a62-4f0d-e1e0-008d6457f512"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();  # Disable dropout during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e062b82-3540-48ce-8eb4-009686d0d16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6516f757-849c-468f-88f7-28ac9debf6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955f9e1a-7bf7-40d8-b1fa-eacabdee8d8e",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec6c217-e429-40c7-ad71-5d0a9da8e487",
   "metadata": {
    "id": "2ec6c217-e429-40c7-ad71-5d0a9da8e487"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Preparing the dataset loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "654fde37-b2a9-4a20-a8d3-0206c056e2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"FaizansLLM/04_pretraining/the-verdict.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379330f1-80f4-4e34-8724-41d892b04cee",
   "metadata": {},
   "source": [
    "- A quick check that the text loaded ok by printing the first and last 100 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6kgJbe4ehI4q",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "6kgJbe4ehI4q",
    "outputId": "9ff31e88-ee37-47e9-ee64-da6eb552f46f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "# First 100 characters\n",
    "print(text_data[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "j2XPde_ThM_e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "j2XPde_ThM_e",
    "outputId": "a900c1b9-9a87-4078-968b-a5721deda5cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it for me! The Strouds stand alone, and happen once--but there's no exterminating our kind of art.\"\n"
     ]
    }
   ],
   "source": [
    "# Last 100 characters\n",
    "print(text_data[-99:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b46a952-d50a-4837-af09-4095698f7fd1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b46a952-d50a-4837-af09-4095698f7fd1",
    "outputId": "c2a25334-21ca-486e-8226-0296e5fc6486"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0959c855-f860-4358-8b98-bc654f047578",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca0116d0-d229-472c-9fbf-ebc229331c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb860488-5453-41d7-9870-23b723f742a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eb860488-5453-41d7-9870-23b723f742a0",
    "outputId": "96b9451a-9557-4126-d1c8-51610a1995ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 4608\n",
      "Validation tokens: 512\n",
      "All tokens: 5120\n"
     ]
    }
   ],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "56f5b0c9-1065-4d67-98b9-010e42fc1e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.987583372328016\n",
      "Validation loss: 10.98110580444336\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9339f8d-00cb-4206-af67-58c32bd72055",
   "metadata": {
    "id": "b9339f8d-00cb-4206-af67-58c32bd72055"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "##  Training Faizan's LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "47d070bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 15\n",
    "initial_lr = 0.0001\n",
    "peak_lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ebb07aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "total_steps = len(train_loader) * n_epochs\n",
    "warmup_steps = int(0.2 * total_steps) # 20% warmup\n",
    "print(warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "33c1a850",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_increment = (peak_lr - initial_lr) / warmup_steps\n",
    "\n",
    "global_step = -1\n",
    "track_lrs = []\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), weight_decay=0.1)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for input_batch, target_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        global_step += 1\n",
    "    \n",
    "        if global_step < warmup_steps:\n",
    "            lr = initial_lr + global_step * lr_increment\n",
    "        else:\n",
    "            lr = peak_lr\n",
    "        \n",
    "        # Apply the calculated learning rate to the optimizer\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "        track_lrs.append(optimizer.param_groups[0][\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "39d64368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "min_lr = 0.1 * initial_lr\n",
    "track_lrs = []\n",
    "\n",
    "lr_increment = (peak_lr - initial_lr) / warmup_steps\n",
    "global_step = -1\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for input_batch, target_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        global_step += 1\n",
    "    \n",
    "        # Adjust the learning rate based on the current phase (warmup or cosine annealing)\n",
    "        if global_step < warmup_steps:\n",
    "            # Linear warmup\n",
    "            lr = initial_lr + global_step * lr_increment  \n",
    "        else:\n",
    "            # Cosine annealing after warmup\n",
    "            progress = ((global_step - warmup_steps) / \n",
    "                        (total_steps - warmup_steps))\n",
    "            lr = min_lr + (peak_lr - min_lr) * 0.5 * (\n",
    "                1 + math.cos(math.pi * progress))\n",
    "        \n",
    "        # Apply the calculated learning rate to the optimizer\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "        track_lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b186914c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from LLMcore import calc_loss_batch\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "\n",
    "loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b5e64ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0839)\n"
     ]
    }
   ],
   "source": [
    "def find_highest_gradient(model):\n",
    "    max_grad = None\n",
    "    for param in model.parameters():\n",
    "        if param.grad is not None:\n",
    "            grad_values = param.grad.data.flatten()\n",
    "            max_grad_param = grad_values.max()\n",
    "            if max_grad is None or max_grad_param > max_grad:\n",
    "                max_grad = max_grad_param\n",
    "    return max_grad\n",
    "\n",
    "print(find_highest_gradient(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "Mtp4gY0ZO-qq",
   "metadata": {
    "id": "Mtp4gY0ZO-qq"
   },
   "outputs": [],
   "source": [
    "Clipping_Flag = False\n",
    "\n",
    "\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device,\n",
    "                n_epochs, eval_freq, eval_iter, start_context, tokenizer,\n",
    "                warmup_steps, initial_lr=3e-05, min_lr=1e-6):\n",
    "\n",
    "    train_losses, val_losses, track_tokens_seen, track_lrs = [], [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Retrieve the maximum learning rate from the optimizer\n",
    "    peak_lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "    # Calculate the total number of iterations in the training process\n",
    "    total_training_steps = len(train_loader) * n_epochs\n",
    "\n",
    "    # Calculate the learning rate increment during the warmup phase\n",
    "    lr_increment = (peak_lr - initial_lr) / warmup_steps\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "\n",
    "            # Adjust the learning rate based on the current phase (warmup or cosine annealing)\n",
    "            if global_step < warmup_steps:\n",
    "                # Linear warmup\n",
    "                lr = initial_lr + global_step * lr_increment  \n",
    "            else:\n",
    "                # Cosine annealing after warmup\n",
    "                progress = ((global_step - warmup_steps) / \n",
    "                            (total_training_steps - warmup_steps))\n",
    "                lr = min_lr + (peak_lr - min_lr) * 0.5 * (1 + math.cos(math.pi * progress))\n",
    "\n",
    "            # Apply the calculated learning rate to the optimizer\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group[\"lr\"] = lr\n",
    "            track_lrs.append(lr)  # Store the current learning rate\n",
    "\n",
    "            # Calculate and backpropagate the loss\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "\n",
    "            # Apply gradient clipping after the warmup phase to avoid exploding gradients\n",
    "            if Clipping_Flag:\n",
    "                if global_step > warmup_steps:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  \n",
    "            else:\n",
    "                if global_step >= warmup_steps:  # the book originally used global_step > warmup_steps, which lead to a skipped clipping step after warmup\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                \n",
    "            optimizer.step()\n",
    "            tokens_seen += input_batch.numel()\n",
    "\n",
    "            # Periodically evaluate the model on the training and validation sets\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader,\n",
    "                    device, eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                # Print the current losses\n",
    "                print(f\"Ep {epoch+1} (Iter {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, \"\n",
    "                      f\"Val loss {val_loss:.3f}\"\n",
    "                )\n",
    "\n",
    "        # Generate and print a sample from the model to monitor progress\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen, track_lrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a301b333-b9d4-4eeb-a212-3a9874e3ac47",
   "metadata": {},
   "source": [
    "I train the LLM using the training function defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3422000b-7aa2-485b-92df-99372cd22311",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3422000b-7aa2-485b-92df-99372cd22311",
    "outputId": "0e046603-908d-4093-8ae5-ef2f632639fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Iter 000000): Train loss 10.924, Val loss 10.939\n",
      "Ep 1 (Iter 000005): Train loss 9.317, Val loss 9.460\n",
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "Ep 2 (Iter 000010): Train loss 7.971, Val loss 8.179\n",
      "Ep 2 (Iter 000015): Train loss 6.589, Val loss 6.852\n",
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "Ep 3 (Iter 000020): Train loss 5.996, Val loss 6.634\n",
      "Ep 3 (Iter 000025): Train loss 6.045, Val loss 6.828\n",
      "Every effort moves you. Gisburn. G. G. I had I had. I had I had I had I had I had I had I had I had I had I had I had. G I had I had I had I had I had I had\n",
      "Ep 4 (Iter 000030): Train loss 6.398, Val loss 7.008\n",
      "Ep 4 (Iter 000035): Train loss 5.966, Val loss 6.972\n",
      "Every effort moves you                                                  \n",
      "Ep 5 (Iter 000040): Train loss 6.082, Val loss 6.867\n",
      "Every effort moves you, and, and the, and, and the, and the the., and the, and the the the, and the the, and the the the the the the the the the the, and the the the the the the the the the\n",
      "Ep 6 (Iter 000045): Train loss 5.841, Val loss 6.781\n",
      "Ep 6 (Iter 000050): Train loss 5.840, Val loss 6.809\n",
      "Every effort moves you, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and\n",
      "Ep 7 (Iter 000055): Train loss 5.383, Val loss 6.699\n",
      "Ep 7 (Iter 000060): Train loss 5.425, Val loss 6.658\n",
      "Every effort moves you.                                                 \n",
      "Ep 8 (Iter 000065): Train loss 4.689, Val loss 6.465\n",
      "Ep 8 (Iter 000070): Train loss 4.307, Val loss 6.516\n",
      "Every effort moves you.                                                 \n",
      "Ep 9 (Iter 000075): Train loss 3.658, Val loss 6.285\n",
      "Ep 9 (Iter 000080): Train loss 3.249, Val loss 6.191\n",
      "Every effort moves you know he was not to have to have to me--I had been.             \"I he was his pictures--and it--and I had the fact, and I felt him, and\n",
      "Ep 10 (Iter 000085): Train loss 2.524, Val loss 6.153\n",
      "Every effort moves you know the fact up-rooms Mrs. Gisburn--as such--and it happened--and that he was not to the Riv to the donkey, and I had always at the house the donkey, and up and down the room, and in\n",
      "Ep 11 (Iter 000090): Train loss 2.450, Val loss 6.170\n",
      "Ep 11 (Iter 000095): Train loss 1.941, Val loss 6.178\n",
      "Every effort moves you know,\" was one of the ax--I felt nervous and uncertain.           He laughed again, and threw back his glory, and as once one had always _rose Dub--because he had alwaysI\n",
      "Ep 12 (Iter 000100): Train loss 1.630, Val loss 6.180\n",
      "Ep 12 (Iter 000105): Train loss 1.654, Val loss 6.162\n",
      "Every effort moves you know,\" was one of the axioms he had been the fact of a flash that he was's an awful simpleton, and Mrs. I was _rose Dub my unexpected discovery; and as I had been the man of the hour. I\n",
      "Ep 13 (Iter 000110): Train loss 1.431, Val loss 6.185\n",
      "Ep 13 (Iter 000115): Train loss 1.294, Val loss 6.188\n",
      "Every effort moves you know,\" was one of the axioms he had been the fact of a and silver of an exquisburn, I had been to the display of his close grayish beard--as if he had the donkey.    \"I\n",
      "Ep 14 (Iter 000120): Train loss 1.514, Val loss 6.173\n",
      "Ep 14 (Iter 000125): Train loss 1.331, Val loss 6.170\n",
      "Every effort moves you know,\" was one of the axioms he had been the frame. \"I was no great, one of Jack's that, and I was back his glory, and as once one had longed to say: \"Be dissatisfied with your\n",
      "Ep 15 (Iter 000130): Train loss 1.232, Val loss 6.175\n",
      "Every effort moves you know,\" was one of the axioms he had been the frame. \"I was no great, one of Jack's that, and I felt back his glory, and as once one had longed to say: \"Be dissatisfied with your\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Note:\n",
    "# Uncomment the following code to calculate the execution time\n",
    "# import time\n",
    "# start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "\n",
    "peak_lr = 0.001  # this was originally set to 5e-4 in the book by mistake\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=peak_lr, weight_decay=0.1)  # the book accidentally omitted the lr assignment\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "n_epochs = 15\n",
    "train_losses, val_losses, tokens_seen, lrs = train_model(\n",
    "    model, train_loader, val_loader, optimizer, device, n_epochs=n_epochs,\n",
    "    eval_freq=5, eval_iter=1, start_context=\"Every effort moves you\",\n",
    "    tokenizer=tokenizer, warmup_steps=warmup_steps, \n",
    "    initial_lr=1e-5, min_lr=1e-5\n",
    ")\n",
    "\n",
    "# Note:\n",
    "# Uncomment the following code to show the execution time\n",
    "# end_time = time.time()\n",
    "# execution_time_minutes = (end_time - start_time) / 60\n",
    "# print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "139885c4-40ed-4765-b307-511d5a967fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0WSRu2i0iHJE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "0WSRu2i0iHJE",
    "outputId": "9d36c61b-517d-4f07-a7e8-4563aff78b11"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2c19ff46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc83ded-5f80-4e1c-bf4d-ccb59999d995",
   "metadata": {},
   "source": [
    "Upon examining the results above, we observe a striking transformation in the model’s generative capabilities. Initially, it churns out a disjointed, unintelligible cascade of words, an erratic stream devoid of coherence. Yet, as training progresses, it begins to construct sentences that, while not flawless, adhere to grammatical conventions with increasing precision.\n",
    "\n",
    "However, a closer analysis of the training and validation losses unveils an inevitable pitfall—overfitting. The model, instead of generalizing language patterns, begins to memorize its training data. If one were to scrutinize its later passages, one would discover entire segments lifted verbatim from the dataset, betraying a reliance on rote recall rather than true linguistic synthesis.\n",
    "\n",
    "There exist decoding strategies—beyond the scope of this workshop—that can temper this tendency to overfit, fostering a more creative and original output. But it is also worth noting that the overfitting we observe here stems largely from the brevity of our training corpus and the relentless cycling through it.\n",
    "\n",
    "Ultimately, this exercise is not about sculpting a state-of-the-art language model; rather, it is a pedagogical endeavor—a window into how an LLM gradually learns to weave meaningful text from raw data. Were we pursuing a production-grade model, the process would span weeks, even months, consuming vast computational resources. Instead, for the sake of efficiency, we later load pretrained weights, allowing us to bypass this Herculean effort and focus on the mechanics of fine-tuning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1f1cd362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text :\n",
      " Every effort moves you know,\" was one of the axioms he had been the frame.\n",
      "\"I was no great, one of Jack\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(model=model, idx=text_to_token_ids(\"Every effort moves you\", tokenizer), max_new_tokens=25, context_size=GPT_CONFIG_124M['context_length'])\n",
    "\n",
    "print(\"Output text :\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6d67c3",
   "metadata": {},
   "source": [
    "**Temprature scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9d38db37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to illustrate \n",
    "vocab = {\n",
    "    \"closer\" : 0,\n",
    "    \"every\" : 1,\n",
    "    \"effort\" : 2,\n",
    "    \"forward\" : 3,\n",
    "    \"inches\" : 4,\n",
    "    \"movesr\" : 5,\n",
    "    \"pizza\" : 6,\n",
    "    \"toward\" : 7,\n",
    "    \"you\" : 8,\n",
    "}\n",
    "\n",
    "inverese_vocab = {v : k for k , v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0e4a74c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits = torch.tensor([4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b6cdfc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "# next_token_id = torch.argmax(probas).item()\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverese_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "133f91d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 x closer\n",
      "2 x every\n",
      "0 x effort\n",
      "544 x forward\n",
      "2 x inches\n",
      "1 x movesr\n",
      "0 x pizza\n",
      "376 x toward\n",
      "4 x you\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123)\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i , freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverese_vocab[i]}\")\n",
    "\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "18eab50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temprature scaling \n",
    "def softmax_with_temprature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f99527de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "temperature = [1 , 0.1 , 5]\n",
    "scaled_probas = [softmax_with_temprature(next_token_logits, T) for T in temperature]\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i , T in enumerate(temperature):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i], bar_width, label=f'Temprature = {T}')\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "aca56d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.3% x closer\n",
      "6.8% x every\n",
      "5.5% x effort\n",
      "22.3% x forward\n",
      "10.2% x inches\n",
      "5.0% x movesr\n",
      "4.3% x pizza\n",
      "21.8% x toward\n",
      "8.8% x you\n"
     ]
    }
   ],
   "source": [
    "# exc 5.1\n",
    "scaled_probas = softmax_with_temprature(next_token_logits, 5)\n",
    "\n",
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123)\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i , freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq/10:.1f}% x {inverese_vocab[i]}\")\n",
    "\n",
    "print_sampled_tokens(scaled_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9427ee2e",
   "metadata": {},
   "source": [
    "### Top-K sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "62b7b928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all logits :  tensor([ 4.5100,  0.8900, -1.9000,  6.7500,  1.6300, -1.6200, -1.8900,  6.2800,\n",
      "         1.7900])\n",
      "top logits :  tensor([6.7500, 6.2800, 4.5100])\n",
      "top postions :  tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "print(\"all logits : \", next_token_logits)\n",
    "print(\"top logits : \", top_logits)\n",
    "print(\"top postions : \", top_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "11b36036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
     ]
    }
   ],
   "source": [
    "# setting exluding values to -inf \n",
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1],\n",
    "    input = torch.tensor(float('-inf')),\n",
    "    other = next_token_logits\n",
    ")\n",
    "\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "37bf1d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e32a4a7",
   "metadata": {},
   "source": [
    "### Altering the text generation function to incorporate Temperature scaling and Top- K "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1cffa506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "20e91844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output text :  Every effort moves you know,\" was one of the axioms he had been the frame.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15, \n",
    "    context_size=GPT_CONFIG_124M['context_length'],\n",
    "    top_k=15,\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "print(\"output text : \", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e5405a",
   "metadata": {},
   "source": [
    "### Saving the model weights in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f479c88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dec70124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load(\"model.pth\", map_location=device))\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
